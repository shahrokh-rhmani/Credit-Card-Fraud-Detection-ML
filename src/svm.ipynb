{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import learning_curve\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the Main Class\n",
    "class CreditCardFraudDetectionSVM:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.results = {}\n",
    "        self.best_params = {}\n",
    "        self.data_loaded = False\n",
    "        \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories for saving plots\"\"\"\n",
    "        os.makedirs('../img/svm/', exist_ok=True)\n",
    "        print(\"Created directories for saving plots\")\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the credit card fraud dataset with multiple fallbacks\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        \n",
    "        # Try multiple URLs and methods\n",
    "        urls = [\n",
    "            \"https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\",\n",
    "            \"https://media.githubusercontent.com/media/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\"\n",
    "        ]\n",
    "        \n",
    "        # Try direct pandas read first\n",
    "        for url in urls:\n",
    "            try:\n",
    "                self.data = pd.read_csv(url)\n",
    "                print(f\"Dataset loaded successfully from: {url}\")\n",
    "                self.data_loaded = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load from {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # If online loading fails, check for local file\n",
    "        if not self.data_loaded:\n",
    "            local_paths = [\n",
    "                \"creditcard.csv\",\n",
    "                \"../creditcard.csv\", \n",
    "                \"../../creditcard.csv\",\n",
    "                \"data/creditcard.csv\"\n",
    "            ]\n",
    "            for local_path in local_paths:\n",
    "                if os.path.exists(local_path):\n",
    "                    try:\n",
    "                        self.data = pd.read_csv(local_path)\n",
    "                        print(f\"Dataset loaded successfully from local file: {local_path}\")\n",
    "                        self.data_loaded = True\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to load from {local_path}: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        # If all methods fail, create better sample data for demonstration\n",
    "        if not self.data_loaded:\n",
    "            print(\"Warning: Could not load dataset from any source. Creating enhanced sample data for demonstration.\")\n",
    "            self.create_enhanced_sample_data()\n",
    "            self.data_loaded = True\n",
    "        \n",
    "        if self.data_loaded:\n",
    "            print(f\"Dataset shape: {self.data.shape}\")\n",
    "            print(f\"Fraud cases: {self.data['Class'].sum()} ({self.data['Class'].mean()*100:.2f}%)\")\n",
    "            \n",
    "            # Separate features and target\n",
    "            X = self.data.drop('Class', axis=1)\n",
    "            y = self.data['Class']\n",
    "            \n",
    "            # Split the data - use smaller test size for faster training\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Scale the features (very important for SVM)\n",
    "            self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "            self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "            \n",
    "            print(f\"Training set: {self.X_train_scaled.shape}\")\n",
    "            print(f\"Test set: {self.X_test_scaled.shape}\")\n",
    "            \n",
    "            return X, y\n",
    "        else:\n",
    "            print(\"Error: Could not load or create dataset.\")\n",
    "            return None, None\n",
    "    \n",
    "    def create_enhanced_sample_data(self):\n",
    "        \"\"\"Create better sample data that works well with SVM\"\"\"\n",
    "        np.random.seed(42)\n",
    "        n_samples = 5000  # Reduced for faster training\n",
    "        n_features = 30\n",
    "        \n",
    "        # Create more realistic features with clear separation\n",
    "        features = np.random.randn(n_samples, n_features)\n",
    "        \n",
    "        # Make fraud cases slightly different from genuine ones\n",
    "        fraud_indices = np.random.choice(n_samples, size=int(n_samples * 0.01), replace=False)  # 1% fraud for better results\n",
    "        \n",
    "        # Modify features for fraud cases to create separation\n",
    "        for idx in fraud_indices:\n",
    "            features[idx, :10] += 2.5  # Make first 10 features significantly different\n",
    "            features[idx, 10:20] -= 1.5  # Make next 10 features different\n",
    "        \n",
    "        target = np.zeros(n_samples)\n",
    "        target[fraud_indices] = 1\n",
    "        \n",
    "        # Create feature names\n",
    "        feature_names = [f'V{i+1}' for i in range(28)] + ['Amount', 'Time']\n",
    "        \n",
    "        self.data = pd.DataFrame(features, columns=feature_names)\n",
    "        self.data['Class'] = target.astype(int)\n",
    "        \n",
    "        print(\"Created enhanced synthetic dataset for demonstration\")\n",
    "        print(\"Note: This is synthetic data. For real results, please download the actual dataset.\")\n",
    "    \n",
    "    def analyze_class_imbalance(self):\n",
    "        \"\"\"Analyze and visualize class imbalance\"\"\"\n",
    "        if not self.data_loaded:\n",
    "            print(\"No data available for analysis.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Class Imbalance Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        class_counts = self.data['Class'].value_counts()\n",
    "        print(f\"Genuine transactions: {class_counts[0]} ({class_counts[0]/len(self.data)*100:.2f}%)\")\n",
    "        print(f\"Fraud transactions: {class_counts[1]} ({class_counts[1]/len(self.data)*100:.2f}%)\")\n",
    "        \n",
    "        # Plot class distribution\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        class_counts.plot(kind='bar', color=['skyblue', 'coral'])\n",
    "        plt.title('Class Distribution')\n",
    "        plt.xlabel('Class (0: Genuine, 1: Fraud)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie(class_counts, labels=['Genuine', 'Fraud'], autopct='%1.2f%%', \n",
    "                colors=['skyblue', 'coral'], startangle=90)\n",
    "        plt.title('Class Distribution (%)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train_svm_fast(self, C=1.0, kernel='rbf', gamma='scale', class_weight=None):\n",
    "        \"\"\"Fast SVM training with limited output\"\"\"\n",
    "        if not self.data_loaded:\n",
    "            print(\"No data available for training.\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"SVM: C={C}, kernel={kernel}, gamma={gamma}, weight={class_weight}\", end=\" | \")\n",
    "        \n",
    "        try:\n",
    "            self.model = SVC(\n",
    "                C=C,\n",
    "                kernel=kernel,\n",
    "                gamma=gamma,\n",
    "                class_weight=class_weight,\n",
    "                random_state=42,\n",
    "                probability=True\n",
    "            )\n",
    "            \n",
    "            self.model.fit(self.X_train_scaled, self.y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_test_pred = self.model.predict(self.X_test_scaled)\n",
    "            y_test_proba = self.model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_accuracy = accuracy_score(self.y_test, y_test_pred)\n",
    "            test_precision = precision_score(self.y_test, y_test_pred, zero_division=0)\n",
    "            test_recall = recall_score(self.y_test, y_test_pred, zero_division=0)\n",
    "            test_f1 = f1_score(self.y_test, y_test_pred, zero_division=0)\n",
    "            \n",
    "            # Store results\n",
    "            param_key = f\"C{C}_kernel{kernel}_gamma{gamma}_weight{class_weight}\"\n",
    "            self.results[param_key] = {\n",
    "                'C': C,\n",
    "                'kernel': kernel,\n",
    "                'gamma': gamma,\n",
    "                'class_weight': class_weight,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_precision': test_precision,\n",
    "                'test_recall': test_recall,\n",
    "                'test_f1': test_f1,\n",
    "                'y_test_proba': y_test_proba\n",
    "            }\n",
    "            \n",
    "            print(f\"F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")\n",
    "            \n",
    "            return self.results[param_key]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def hyperparameter_tuning_fast(self):\n",
    "        \"\"\"Fast hyperparameter tuning for SVM with better parameters\"\"\"\n",
    "        if not self.data_loaded:\n",
    "            print(\"No data available for hyperparameter tuning.\")\n",
    "            return None\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Fast Hyperparameter Tuning\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Better parameter grid for imbalanced data\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 0.01, 0.1, 1],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "        \n",
    "        best_score = -1\n",
    "        best_params = {}\n",
    "        \n",
    "        total_combinations = (len(param_grid['C']) * \n",
    "                            len(param_grid['kernel']) * \n",
    "                            len(param_grid['gamma']) * \n",
    "                            len(param_grid['class_weight']))\n",
    "        current_combination = 0\n",
    "        \n",
    "        print(f\"Testing {total_combinations} parameter combinations\")\n",
    "        print(\"Format: SVM: C=X, kernel=Y, gamma=Z, weight=W | F1: A, Precision: B, Recall: C\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for C in param_grid['C']:\n",
    "            for kernel in param_grid['kernel']:\n",
    "                for gamma in param_grid['gamma']:\n",
    "                    for class_weight in param_grid['class_weight']:\n",
    "                        current_combination += 1\n",
    "                        \n",
    "                        # Skip invalid combinations\n",
    "                        if kernel == 'linear' and gamma != 'scale':\n",
    "                            continue\n",
    "                            \n",
    "                        results = self.train_svm_fast(\n",
    "                            C=C,\n",
    "                            kernel=kernel,\n",
    "                            gamma=gamma,\n",
    "                            class_weight=class_weight\n",
    "                        )\n",
    "                        \n",
    "                        if results is not None:\n",
    "                            # Use F1-score as the main metric\n",
    "                            current_score = results['test_f1']\n",
    "                            if current_score > best_score:\n",
    "                                best_score = current_score\n",
    "                                best_params = {\n",
    "                                    'C': C,\n",
    "                                    'kernel': kernel,\n",
    "                                    'gamma': gamma,\n",
    "                                    'class_weight': class_weight,\n",
    "                                    'results': results\n",
    "                                }\n",
    "                                print(\" *** New best! ***\")\n",
    "        \n",
    "        if best_params and best_score > 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"*** BEST PARAMETERS FOUND ***\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Best C: {best_params['C']}\")\n",
    "            print(f\"Best kernel: {best_params['kernel']}\")\n",
    "            print(f\"Best gamma: {best_params['gamma']}\")\n",
    "            print(f\"Best class_weight: {best_params['class_weight']}\")\n",
    "            print(f\"Best F1-Score: {best_score:.4f}\")\n",
    "            \n",
    "            self.best_params = best_params\n",
    "            return best_params\n",
    "        else:\n",
    "            print(\"\\nNo valid results obtained from hyperparameter tuning.\")\n",
    "            print(\"This might be due to:\")\n",
    "            print(\"1. Using synthetic data with poor separation\")\n",
    "            print(\"2. SVM struggling with the current data distribution\")\n",
    "            print(\"3. Try downloading the real dataset for better results\")\n",
    "            return None\n",
    "    \n",
    "    def plot_hyperparameter_analysis(self):\n",
    "        \"\"\"Plot analysis of hyperparameter effects with error handling\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to plot. Please train the model first.\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        C_values = []\n",
    "        kernels = []\n",
    "        gammas = []\n",
    "        class_weights = []\n",
    "        test_f1_scores = []\n",
    "        test_precisions = []\n",
    "        test_recalls = []\n",
    "        \n",
    "        for param_key, result in self.results.items():\n",
    "            C_values.append(result['C'])\n",
    "            kernels.append(result['kernel'])\n",
    "            gammas.append(str(result['gamma']))\n",
    "            class_weights.append(str(result['class_weight']))\n",
    "            test_f1_scores.append(result['test_f1'])\n",
    "            test_precisions.append(result['test_precision'])\n",
    "            test_recalls.append(result['test_recall'])\n",
    "        \n",
    "        # Create DataFrame for easier plotting\n",
    "        results_df = pd.DataFrame({\n",
    "            'C': C_values,\n",
    "            'kernel': kernels,\n",
    "            'gamma': gammas,\n",
    "            'class_weight': class_weights,\n",
    "            'f1_score': test_f1_scores,\n",
    "            'precision': test_precisions,\n",
    "            'recall': test_recalls\n",
    "        })\n",
    "        \n",
    "        # Filter out results with zero scores for better visualization\n",
    "        valid_results_df = results_df[results_df['f1_score'] > 0]\n",
    "        \n",
    "        if len(valid_results_df) == 0:\n",
    "            print(\"No valid results with positive F1-scores to plot.\")\n",
    "            return\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: C parameter vs F1-score for different kernels\n",
    "        try:\n",
    "            for kernel in ['linear', 'rbf']:\n",
    "                kernel_data = valid_results_df[valid_results_df['kernel'] == kernel]\n",
    "                if len(kernel_data) > 0:\n",
    "                    C_groups = kernel_data.groupby('C')['f1_score'].mean().reset_index()\n",
    "                    axes[0,0].plot(C_groups['C'], C_groups['f1_score'], \n",
    "                                  marker='o', label=f'{kernel} kernel', linewidth=2)\n",
    "            axes[0,0].set_xlabel('C (Regularization Parameter)')\n",
    "            axes[0,0].set_ylabel('F1-Score')\n",
    "            axes[0,0].set_title('C Parameter vs F1-Score (by Kernel)')\n",
    "            axes[0,0].set_xscale('log')\n",
    "            axes[0,0].legend()\n",
    "            axes[0,0].grid(True, alpha=0.3)\n",
    "        except Exception as e:\n",
    "            axes[0,0].text(0.5, 0.5, 'No data for this plot', ha='center', va='center')\n",
    "            axes[0,0].set_title('C Parameter vs F1-Score (No Data)')\n",
    "        \n",
    "        # Plot 2: Kernel comparison\n",
    "        try:\n",
    "            kernel_data = valid_results_df[['kernel', 'f1_score', 'precision', 'recall']].copy()\n",
    "            kernel_groups = kernel_data.groupby('kernel').agg({\n",
    "                'f1_score': 'mean',\n",
    "                'precision': 'mean',\n",
    "                'recall': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            metrics = ['f1_score', 'precision', 'recall']\n",
    "            metric_names = ['F1-Score', 'Precision', 'Recall']\n",
    "            x_pos = np.arange(len(metrics))\n",
    "            width = 0.35\n",
    "            \n",
    "            for i, kernel in enumerate(['linear', 'rbf']):\n",
    "                if kernel in kernel_groups['kernel'].values:\n",
    "                    kernel_scores = [kernel_groups[kernel_groups['kernel'] == kernel][metric].values[0] for metric in metrics]\n",
    "                    axes[0,1].bar(x_pos + i*width - width, kernel_scores, width, label=kernel, alpha=0.8)\n",
    "            \n",
    "            axes[0,1].set_xlabel('Metrics')\n",
    "            axes[0,1].set_ylabel('Score')\n",
    "            axes[0,1].set_title('Kernel Type Comparison')\n",
    "            axes[0,1].set_xticks(x_pos)\n",
    "            axes[0,1].set_xticklabels(metric_names)\n",
    "            axes[0,1].legend()\n",
    "            axes[0,1].grid(True, alpha=0.3)\n",
    "        except Exception as e:\n",
    "            axes[0,1].text(0.5, 0.5, 'No data for this plot', ha='center', va='center')\n",
    "            axes[0,1].set_title('Kernel Comparison (No Data)')\n",
    "        \n",
    "        # Plot 3: Class weight comparison with error handling\n",
    "        try:\n",
    "            weight_data = valid_results_df[['class_weight', 'f1_score', 'precision', 'recall']].copy()\n",
    "            weight_groups = weight_data.groupby('class_weight').agg({\n",
    "                'f1_score': 'mean',\n",
    "                'precision': 'mean',\n",
    "                'recall': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            metrics = ['f1_score', 'precision', 'recall']\n",
    "            metric_names = ['F1-Score', 'Precision', 'Recall']\n",
    "            x_pos = np.arange(len(metrics))\n",
    "            width = 0.35\n",
    "            \n",
    "            # Safe value extraction\n",
    "            none_scores = []\n",
    "            balanced_scores = []\n",
    "            \n",
    "            for metric in metrics:\n",
    "                none_data = weight_groups[weight_groups['class_weight'] == 'None']\n",
    "                balanced_data = weight_groups[weight_groups['class_weight'] == 'balanced']\n",
    "                \n",
    "                none_scores.append(none_data[metric].values[0] if len(none_data) > 0 else 0)\n",
    "                balanced_scores.append(balanced_data[metric].values[0] if len(balanced_data) > 0 else 0)\n",
    "            \n",
    "            axes[1,0].bar(x_pos - width/2, none_scores, width, label='None', alpha=0.8, color='blue')\n",
    "            axes[1,0].bar(x_pos + width/2, balanced_scores, width, label='Balanced', alpha=0.8, color='red')\n",
    "            axes[1,0].set_xlabel('Metrics')\n",
    "            axes[1,0].set_ylabel('Score')\n",
    "            axes[1,0].set_title('Class Weight Strategy Comparison')\n",
    "            axes[1,0].set_xticks(x_pos)\n",
    "            axes[1,0].set_xticklabels(metric_names)\n",
    "            axes[1,0].legend()\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "        except Exception as e:\n",
    "            axes[1,0].text(0.5, 0.5, 'No data for this plot', ha='center', va='center')\n",
    "            axes[1,0].set_title('Class Weight Comparison (No Data)')\n",
    "        \n",
    "        # Plot 4: Precision-Recall trade-off\n",
    "        try:\n",
    "            if len(valid_results_df) > 0:\n",
    "                scatter = axes[1,1].scatter(valid_results_df['precision'], valid_results_df['recall'], \n",
    "                                           c=valid_results_df['f1_score'], s=50, alpha=0.7, cmap='viridis')\n",
    "                axes[1,1].set_xlabel('Precision')\n",
    "                axes[1,1].set_ylabel('Recall')\n",
    "                axes[1,1].set_title('Precision-Recall Trade-off (Color: F1-Score)')\n",
    "                axes[1,1].grid(True, alpha=0.3)\n",
    "                plt.colorbar(scatter, ax=axes[1,1], label='F1-Score')\n",
    "            else:\n",
    "                axes[1,1].text(0.5, 0.5, 'No data for this plot', ha='center', va='center')\n",
    "                axes[1,1].set_title('Precision-Recall Trade-off (No Data)')\n",
    "        except Exception as e:\n",
    "            axes[1,1].text(0.5, 0.5, 'No data for this plot', ha='center', va='center')\n",
    "            axes[1,1].set_title('Precision-Recall Trade-off (No Data)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_final_results(self):\n",
    "        \"\"\"Plot final results with best parameters\"\"\"\n",
    "        if not self.best_params or not self.data_loaded:\n",
    "            print(\"No best parameters found or no data available. Please run hyperparameter tuning first.\")\n",
    "            return\n",
    "        \n",
    "        # Train best model with probabilities\n",
    "        best_C = self.best_params['C']\n",
    "        best_kernel = self.best_params['kernel']\n",
    "        best_gamma = self.best_params['gamma']\n",
    "        best_class_weight = self.best_params['class_weight']\n",
    "        \n",
    "        print(f\"\\nTraining final model with best parameters...\")\n",
    "        \n",
    "        self.model = SVC(\n",
    "            C=best_C,\n",
    "            kernel=best_kernel,\n",
    "            gamma=best_gamma,\n",
    "            class_weight=best_class_weight,\n",
    "            random_state=42,\n",
    "            probability=True\n",
    "        )\n",
    "        self.model.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(self.X_test_scaled)\n",
    "        y_proba = self.model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Create comprehensive results plot\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Confusion Matrix\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                   xticklabels=['Genuine', 'Fraud'], \n",
    "                   yticklabels=['Genuine', 'Fraud'])\n",
    "        ax1.set_title(f'Confusion Matrix\\nC={best_C}, kernel={best_kernel}, gamma={best_gamma}')\n",
    "        ax1.set_xlabel('Predicted')\n",
    "        ax1.set_ylabel('Actual')\n",
    "        \n",
    "        # Plot 2: ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "        ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "        ax2.set_xlim([0.0, 1.0])\n",
    "        ax2.set_ylim([0.0, 1.05])\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive Rate')\n",
    "        ax2.set_title('ROC Curve')\n",
    "        ax2.legend(loc=\"lower right\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, y_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        ax3.plot(recall, precision, color='green', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
    "        ax3.set_xlim([0.0, 1.0])\n",
    "        ax3.set_ylim([0.0, 1.05])\n",
    "        ax3.set_xlabel('Recall')\n",
    "        ax3.set_ylabel('Precision')\n",
    "        ax3.set_title('Precision-Recall Curve')\n",
    "        ax3.legend(loc=\"lower left\")\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Performance Metrics\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "        scores = [\n",
    "            accuracy_score(self.y_test, y_pred),\n",
    "            precision_score(self.y_test, y_pred, zero_division=0),\n",
    "            recall_score(self.y_test, y_pred, zero_division=0),\n",
    "            f1_score(self.y_test, y_pred, zero_division=0)\n",
    "        ]\n",
    "        \n",
    "        bars = ax4.bar(metrics, scores, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "        ax4.set_ylabel('Score')\n",
    "        ax4.set_title('Performance Metrics')\n",
    "        ax4.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_comprehensive_report(self):\n",
    "        \"\"\"Print comprehensive classification report\"\"\"\n",
    "        if not self.best_params or not self.data_loaded:\n",
    "            print(\"No best parameters found or no data available. Please run hyperparameter tuning first.\")\n",
    "            return\n",
    "        \n",
    "        best_C = self.best_params['C']\n",
    "        best_kernel = self.best_params['kernel']\n",
    "        best_gamma = self.best_params['gamma']\n",
    "        best_class_weight = self.best_params['class_weight']\n",
    "        best_results = self.best_params['results']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE CLASSIFICATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Best Parameters:\")\n",
    "        print(f\"  - C: {best_C}\")\n",
    "        print(f\"  - kernel: {best_kernel}\")\n",
    "        print(f\"  - gamma: {best_gamma}\")\n",
    "        print(f\"  - class_weight: {best_class_weight}\")\n",
    "        \n",
    "        # Train model for classification report\n",
    "        self.model = SVC(\n",
    "            C=best_C,\n",
    "            kernel=best_kernel,\n",
    "            gamma=best_gamma,\n",
    "            class_weight=best_class_weight,\n",
    "            random_state=42,\n",
    "            probability=True\n",
    "        )\n",
    "        self.model.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        y_pred = self.model.predict(self.X_test_scaled)\n",
    "        print(\"\\n\" + classification_report(self.y_test, y_pred, target_names=['Genuine', 'Fraud']))\n",
    "        \n",
    "        # Cross-validation scores with reduced folds\n",
    "        try:\n",
    "            cv_scores_f1 = cross_val_score(self.model, self.X_train_scaled, self.y_train, cv=3, scoring='f1')\n",
    "            cv_scores_accuracy = cross_val_score(self.model, self.X_train_scaled, self.y_train, cv=3, scoring='accuracy')\n",
    "            cv_scores_precision = cross_val_score(self.model, self.X_train_scaled, self.y_train, cv=3, scoring='precision')\n",
    "            cv_scores_recall = cross_val_score(self.model, self.X_train_scaled, self.y_train, cv=3, scoring='recall')\n",
    "            \n",
    "            print(f\"\\nCross-validation Scores (3-fold):\")\n",
    "            print(f\"F1:       {cv_scores_f1.mean():.4f} (+/- {cv_scores_f1.std() * 2:.4f})\")\n",
    "            print(f\"Accuracy: {cv_scores_accuracy.mean():.4f} (+/- {cv_scores_accuracy.std() * 2:.4f})\")\n",
    "            print(f\"Precision: {cv_scores_precision.mean():.4f} (+/- {cv_scores_precision.std() * 2:.4f})\")\n",
    "            print(f\"Recall:    {cv_scores_recall.mean():.4f} (+/- {cv_scores_recall.std() * 2:.4f})\")\n",
    "        except:\n",
    "            print(\"\\nCross-validation failed (possibly due to no positive class predictions)\")\n",
    "        \n",
    "        print(f\"\\nTest Set Performance:\")\n",
    "        print(f\"Accuracy:  {best_results['test_accuracy']:.4f}\")\n",
    "        print(f\"Precision: {best_results['test_precision']:.4f}\")\n",
    "        print(f\"Recall:    {best_results['test_recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {best_results['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Run the Complete Pipeline\n",
    "print(\"Starting Credit Card Fraud Detection with SVM...\")\n",
    "\n",
    "# Initialize the fraud detection system\n",
    "fraud_detector = CreditCardFraudDetectionSVM()\n",
    "\n",
    "# Create directories first\n",
    "fraud_detector.create_directories()\n",
    "\n",
    "# Load and preprocess data\n",
    "data_loaded = fraud_detector.load_data()\n",
    "\n",
    "if not fraud_detector.data_loaded:\n",
    "    print(\"\\nTo use the real dataset, please:\")\n",
    "    print(\"1. Download 'creditcard.csv' from Kaggle: https://www.kaggle.com/mlg-ulb/creditcardfraud\")\n",
    "    print(\"2. Place it in the same directory as this script\")\n",
    "    print(\"3. Run the script again\")\n",
    "    print(\"\\nFor now, using enhanced synthetic data for demonstration...\")\n",
    "\n",
    "# Analyze class imbalance\n",
    "fraud_detector.analyze_class_imbalance()\n",
    "\n",
    "# Perform focused hyperparameter tuning\n",
    "best_params = fraud_detector.hyperparameter_tuning_fast()\n",
    "\n",
    "if best_params:\n",
    "    # Plot hyperparameter analysis\n",
    "    fraud_detector.plot_hyperparameter_analysis()\n",
    "    \n",
    "    # Plot final results\n",
    "    fraud_detector.plot_final_results()\n",
    "    \n",
    "    # Print comprehensive report\n",
    "    fraud_detector.print_comprehensive_report()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL MODEL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    results = best_params['results']\n",
    "    print(f\"Best Parameters:\")\n",
    "    print(f\"  - C: {best_params['C']}\")\n",
    "    print(f\"  - kernel: {best_params['kernel']}\")\n",
    "    print(f\"  - gamma: {best_params['gamma']}\")\n",
    "    print(f\"  - class_weight: {best_params['class_weight']}\")\n",
    "    print(f\"\\nTest Performance:\")\n",
    "    print(f\"  - Accuracy:  {results['test_accuracy']:.4f}\")\n",
    "    print(f\"  - Precision: {results['test_precision']:.4f}\")\n",
    "    print(f\"  - Recall:    {results['test_recall']:.4f}\")\n",
    "    print(f\"  - F1-Score:  {results['test_f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SVM PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Key Insights:\")\n",
    "    print(\"- SVM performance heavily depends on proper parameter tuning\")\n",
    "    print(\"- RBF kernel often works well for complex, non-linear problems\")\n",
    "    print(\"- C parameter controls the trade-off between margin and classification error\")\n",
    "    print(\"- Gamma parameter controls the influence of individual training examples\")\n",
    "    print(\"- Class weighting helps handle imbalanced datasets\")\n",
    "    print(\"- Feature scaling is crucial for SVM performance\")\n",
    "else:\n",
    "    print(\"\\nHyperparameter tuning failed or produced poor results.\")\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"1. Download the real dataset for better results\")\n",
    "    print(\"2. Try different parameter ranges\")\n",
    "    print(\"3. Consider using a different algorithm for this data\")\n",
    "\n",
    "print(\"All steps completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
